### Povezovalna pravila

- dobimo opise "zanimivih" podmnožic primerov
- želimo najti strukturo v podatkih tako, da najdemo relacije med atributi
- želimo najti zanimiva in uporabna povezovalna pravila
- mešanica nadzorovanega in nenadzorovanega učenja
- išče povezave med atributi in njihovimi vrednostmi
- dobimo pravila oblike:
	- X => Y
	- če veljajo lastnosti X, potem veljajo tudi lastnosti Y
	- ![600](../../Images3/Pasted%20image%2020250424135420.png)
	- Y ni nujno razred
	- če rečemo, da mora biti Y razred, potem dobimo klasifikacijska povezovalna pravila

Ocenjevanje pravil:
- podpora (support, S):
	- delež vseh primerov, kjer velja tako X kot tudi Y
	- delež primerov za katere velja X in Y = verjetnost, da bomo imeli tako lastnost X kot Y, $p(X \wedge Y)$
	- pove nam, kolikšen delež vseh podatkov opiše
	- S(X => Y) = št.(X in Y) / N
	- visoka podpora = "močan"/pogost vzorec
- zaupanje (conifdence, C)
	- napovedna vrednost (moč) tega pravila
	- če je zaupanje visoko, je v veliko primerih: kjer velja X, velja tudi Y
	- visoko zaupanje = "močno" napoveden vzorec oz. pravilo
	- C(X => Y) = št.(X in Y) / št.(X) = $p(Y | X)$
- postopek iskanja povezovalnih pravil:
	- 1. postavi spodnjo mejo za S (podporo), npr. S(X => Y) > 0.8 (80% primerov mora vsebovati lastnosti X in Y - to je zelo visoka meja, tako da mogoče nam tako pravilo ne bo niti dalo nekega novega znanja)
	- 2. pridobljena pravila uredimo po padajočem C (zaupanju), izberi pravilo z najvišjim zaupanjem, da dobimo pravila, ki imajo dejansko napovedno vrednost
	- želimo, da sta podpora in zaupanje visoki
- daljše kot je pravilo, manj bo vzorcev, ki mu bodo ustrezali
	- ![500](../../Images3/Pasted%20image%2020250508113632.png)
	- pri tem primeru je premalo vzorcev, ki imajo X, zato mu niti ni toliko za zaupati

Analiza košarice:
- kako postaviti izdelke po trgovini, da kupimo stvari, ki jih ne rabimo
- redek (sparse) zapis transakcij - samo napišemo, kaj je nekdo kupil, ni smiselno dajati tega v matriko z vsemi izdelki, ker bo ogromno ničel
- npr. ugotoviš, da ljudje skupaj kupujejo plenice in pivo - jiju daš skupaj ali pa pivo na poti do plenic
- iskanje povezovalnih pravil:
	- d izdelkov => $R = 3^d- 2^{d+1} + 1$ (število pravil)
	- problem eksponenta - pri d=100 je R = $5 \cdot 10^{47}$
	- 1. iskanje pogostih podmnožic, "preštejemo" primere, ki imajo iste lastnosti, npr. pivo, plenice in mleko se velikokrat pojavijo skupaj:
		- uporabimo nek pameten algoritem za iskanje, npr. A-priori
			- zahteve so vedno večje, vedno več lastnosti hočemo v vrsticah, zato bo vedno manj vrstic
			- ![500](../../Images3/Pasted%20image%2020250508115426.png)
			- na vrhu je null, kar pomeni da bo vse true, potem ostri zahteve
			- te množice pravil porežemo - ko dobimo eno množico, ki ima premajhno podporo, porežemo vsa pravila pod njo
			- ![500](../../Images3/Pasted%20image%2020250508115648.png)
			- če imamo 200 izdelkov (d=200), potem imamo na drugem nivoju toliko vozlišč, kot je vseh parov - št. parov = d(d-1)/2 = 19900, in potem se na tem nivoju že veliko vej poreže (če imajo premajhno podporo) in se kombinatorika zmanjša
			- hitro požagamo neperspektivne veje
	- 2. kombinatorično zgradimo vsa možna pravila:
		- ![500](../../Images3/Pasted%20image%2020250508115056.png)
		- izmed teh pravil izberemo tiste, ki imajo dovolj veliko zaupanje, ostale zavržemo
	- v Orange Frequent Itemsets in Association Rules
	- če povečamo minimalno podporo, se bo drevo hitreje začelo rezati in bomo dobili manj pogostih podmnožic, torej dobimo tudi manj možnih povezovalnih pravil
	- velikokrat dobimo lastnosti, ki imajo preprost X in Y, ker imajo ta večjo podporo
	- ko dobimo neka pravila, ki zadostujejo podpori in zaupanju, rabimo nekega eksperta, ki nam pove, katera pravila so dejansko zanimiva in uporabna
	- primer klasifikacije živali:
		- če gledamo samo klisfikacijska povezovalna pravila, bomo vedno dobili pravila, ki opisujejo sesalce - če hočemo še za ostale, moramo seslace filtrirati ven; vsakič ko odstranimo ven del podatkov, izgubimo to, da npr. ena lastnost velja in za ptice in sesalce in ocena zaupanja ne bo primerna
		- podporo moramo dati na 40% ali manj, ker je sesalcev toliko
		- povezovalna pravila so dobra samo zato, da najdemo nek močen vzorec v podatkih (npr. milk=1 => type=mammal); ostala pravila o sesalcih nam bolj podrobno povejo, kako se sesalci ločijo od ostalih vrst, ampak milk=1 nam najbolje pove, da je žival sesalec, ker vsa ostala pravila to vsebujejo noter

IF-THEN pravila:
- klasifikacijska pravila (THEN je razred)
- CN2 algoritem (pokrivni algoritem):
	- podoben A-priori - postopoma ostri pravila v snopu, začne z resnico
	- ko najde "dobro" pravilo, odstani pokrite primere (ali zmanjša utež pokritim primerom) - pokriti primeri hitro odpadejo
	- ustavi se, ko pokrije vse primere
	- je boljši, ko iščemo opis celotnih podatkov
- pravila so urejena in jih beremo od zgoraj navzdol:
	- ko dobimo nek nov primer, najprej beremo prvo pravilo in če mu zadošča končamo, sicer gremo gledati pravila naprej
- lahko imamo neurejena pravila:
	- moramo vse pregledati in vsako pravilo "glasuje" za nek razred, na koncu vzamemo tisti razred, ki je bil največkrat izglasovan (ima največjo verjetnost)
	- je malo težje za razumevanje, ker rabimo vsa pravila hkrati gledati
- dobimo boljše opise kot pri povezovalnih pravilih, ker nam povezovalna pravila opišejo samo največji vzorec v podatkih (tisti, ki ima največji support)

### Priporočilni sistemi in faktorizacija (razcep) matrik

- modele strojnega učenja gradimo zato, da najdemo neke vzorce med podatki, skupine atributov, ki nam povejo neke stvari o predmetih, ki jih opisujemo
- matrična faktorizacija:
	- opisujemo podatke z enostavnim modelom (dve matriki)
	- cilj faktorizacija: poišči takšen W in H (model podatkov), da bo nnjun produkt enak X (podatki)
	- model = zgoščeni/kompresirani podatki
	- pri X imamo $m \cdot n$ celic (vsi podatki), pri modelu pa $m \cdot k + k \cdot n = k(m +n)$ celic
	- hočemo, da je velikost modela bistveno manjša od velikosti podatkov ($|\text{model}| << |\text{podatki}|$), torej $k << min(m, n)$
	- želimo:
		- 1. model je majhen v primerjavi z velikostjo podatkov
		- 2. model dobro opiše podatke => $||X - W\cdot H||$ razlika mora biti čim manjša, da model dobro opisuje podatke
- v praksi gre k do največ 50 ali 100

Iskanje modela (W in H):
- optimizacijski problem (podobno kot pri nevronskih mrežah optimiziramo uteži)
- gradientni sestop (gradial descent), da se razlika med $X$ in $W \cdot H$ manjša
- v bistvu delamo gručenje, W in H sta predstavitvi pripadnosti gručam; W = gručenje vrstic (npr. genov), H = gručenje stolpcev (npr. oseb)
- delamo gručenje po vrsticah in hkrati po stolpcih
- lahko naredimo neko naključno matriko, po katerih bomo iskali vzorce, ki se ji ujemajo - če to matriko množimo samo s sabo, dobimo naključne podatke, ki imajo nek vzorec
- 1. naredi W in H z naključnimi vrednostmi
	- bolje je, če vzamemo nekaj stolpcev iz X za W in nekaj vrstic iz X za H - bo hitreje konvergiralo
- 2. izračunaj $X_{app} = W \cdot H$
- 3. izračunaj napako $X - X_{app}$ (pravi podatki - aproksimirani podatki)
- 4. delno popravimo W in H, vsako celico popravimo sorazmerno z napako
	- ![200](../../Images3/Pasted%20image%2020250508133713.png)
	- dw so napake v vrsticah, dh so napake v stolpcih
	- to je gradienti sestop
	- eta = kako hitro skušamo popraviti napako, ne želimo prehitro in ne prepočasi, da ne preskočimo pravilne rešitve
- 5. ponavljaj do konvergence (najbrž ne bomo prišli do konvergence, ampak se ustavimo, ko se napaka dovolj malo spreminja)

- $W_i$ = latenten opis vrstice i
- $H_j$ = latenten opis stolpca j
- latenten opis = vektor števil, za katere zares ne vemo kaj pomenijo (kot pri nevronskih mrežah vložitev/embedding)

Kako določiti parameter k:
- koliko stolpcev v W oz. vrstic v H moramo imeti
- želimo čim manjši k, ki izgubi čim manj informacij o podatih
- podobno kot pri K-means je odvisno od inicializacije k-ja to, kakšen rezultat bomo dobili, zato je smiselno večkrat ponoviti algoritem z istim k
- dobimo "hockey-stick" pri scree-plot
- ![500](../../Images3/Pasted%20image%2020250508135113.png)
- ^okoli k=3 in k=6 dobimo "prevoj" oz. komolec - to je optimalen k, ker je dovolj majhen, da bo model majhen, če bi k povečali ne bi imeli dobrega tradeoffa med dodatno natančnostjo in dodatno velikostjo modela

- tudi napake nam lahko nekaj povejo o podatkih, to je tisto, česar model ne zna razložiti - potencialni osamelci ali merska napaka

primer iris:
- ![400](../../Images3/Pasted%20image%2020250515112423.png)
- če pogledamo W in H ločeno, vidimo, da nam drugi stolpec v W sovpada z razredom C
- v H vidimo, da sta si prva dva stolpca bolj podobna in druga dva stolpca sta si bolj podobna - venčni listi so ponavadi večji od cvetnih listov (dva stolpca za venčnain dva za cvetne liste)
- $H_i$ = atributi listov
- ![500](../../Images3/Pasted%20image%2020250515112838.png)
- ^ iz te slike ne vidimo najbolje, da so si cvetni listi in venčni listi med sabo podobni, ker imamo premalo atributov in imamo velike razdalje med njimi
- lahko damo na obe osi isti faktor (isti stolpec za W in isto vrstico za H)
- ![600](../../Images3/Pasted%20image%2020250515113128.png)
- ^vidimo povezavo med petal in sepal
- s tem lahko skušamo ugotoviti pomen teh latentnih opisov
- ![500](../../Images3/Pasted%20image%2020250515113207.png)
- ^faktor 1 bolje loči razred in slabše atribute

- v aproksimiranih podatkih je manj šuma
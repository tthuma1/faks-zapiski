- vmesno poročilo:
	- osnovna statistika, vizualizacije
	- kje so osamelci, kaj šteješ pod "večino" podatkov
	- kako je bilo treba podatke transformirati
	- ali so bili podatki uporabni ali ne
	- raje manj napisano in da slika več pove
	- ali je preveč podatkov in bomo morali delati vzorce
- končno poročilo:
	- v streamlit predstaviš glavno stvar, ne nujno vse, kar si naredil

- pri hierarhičnem gručenju združujemo od spodaj navzgor
- pri k-means probamo zajeti podatke, ki so najbolj skupaj 

- problem koliko skupin imamo:
	- pri hierarhičnem gručenju

- če večkrat poženemo k-means dobimo različne skupine, odvisno od postavitve centroidov:
	- rešitev: večkrat poženeš k-means in poiščeš konsenz
- pri hierarhičnem gručenju dobimo samo neko drevo:
	- problem je, da je lahko šum v podatkih - slučajno sta dva primere najbližje, čeprav ne spadata zares v isto gručo

- zanesljivost gručenja, v povezavi s šumom v podatkih:
	- pri k-means poženemo večkrat in pogledamo konsenz; izvor nezanesljivosti je random inicializacija
	- pri hierarhičnem gručenju dobimo eno drevo in ni zares nobenega konsenza
- obe metodi gručenja se poslabšata zaradi šuma v podatkih - vpliv lahko zmanjšamo z bootstrapom:
	- na podatke gledamo na različne načine
	- uvedemo naključnost v funkcijo razdalje
	- ponavljamo s spremenjenimi razdaljami med primeri
	- dosežemo konsenz
	- dendrogram ti bo povedal, kolikokrat se je pri iteracijah zgodila neka prikazana ta delitev med n ponovitvami
	- moramo npr. 100-krat ponoviti gručenje

- kako merimo razdaljo:
	- kosinusna razdalja je bolje za visokodimenzionalne podatke
	- velikokrat uporabimo Evklidsko razdaljo

- linkage:
	- min -  ko nočemo, da se stvari prekrivajo
	- max - ko nam je vseeno, če se stvari prekrivajo, ker nas zanimajo ekstremi
	- avg - ko ne vemo zares, kaj hočemo dobiti - skupino so še vedno dovolj narazen in se ne prekrivajo preveč
	- wardovo združevanje:
		- zagotavlja da so pripadniki gruče čim bolj enako podobni med sabo
		- podobnost je lahko precej velika, ampak če so si vsi enako podobni, potem je to dobro
		- dobro ko imamo eno področje, kjer so primeru blizu skupaj in področje, kjer so primeri daleč narazen
		- različno "gosta" področja primerov ga ne zmotijo, medtem ko npr. k-means to zmoti
		- npr. imaš smučarje, ki kupujejo podobne stvari in ljubitelje narave, ki so si sicer podobni, ampak so podobnosti bolj narazen

Kvaliteta gručenja:
- kakšen mora biti dendrogram, da rečemo, da je gručenje dobro:
	- spodaj so gruče čim bolj skupaj (čim manjša razdalja znotraj gruče), zgoraj je razdalja med gručami čim večja (čim večja razdalja med gručami) - rečemo, da je oblika metle/grabelj s čim manjšimi ščetinami

Mere kvalitete gručenja:
- 100+ različnih mer
- vsaka mera je predlagana na podlagi nekih predpostavk nad podatki, ampak ko dobiš neke podatke ne veš, katera je zares najboljša
- če ničesar ne veš o podatkih, je najboljša ocena silhuetni koeficient
	- $S_i = \frac{b_i - a_i}{max(a_i, \; b_i)}$
	- $a_i =$ razdalja do pripadnikov iste gruče kot je vzorec i
	- $b_i =$ razdalja do najbližje gruče
	- a = neka gruča, b = najbljižja druga gruča
	- i = vzorec
	- ko bo razdalja do pripadnikov isto gruče kot je vzorec i čim manjša bistveno večja kot razdalja do najbližje gruče
	- $-1 \leq S_i \leq 1$
	- želimo, da je $S_i$ čim bližje 1, če je $S_i$ negativen, pomeni, da tisti vzorec ne sodi dobro v dano skupino
	- $S = \frac{1}{n} \sum_{n}{S_i}$
	- S = povprečje vseh silhuetnih razdalj

### Nadzorovano modeliranje

- imamo podano ciljno spremenljivko (atribut, razred), imamo eno ali več ciljnih spremenljivk
- pri nenadzorovanem (gr)učenju ne vemo, kateri atribut je ciljni

Algoritmi nadzorovanega učenje:
- klasifikacijska drevesa
- naivni Bayes
- naključni gozdovi:
	- več prisilno manjših klasifikacijskih dreves, v korenu dreves ni najboljši atribut
	- ansambel plitvih dreves
- metoda podpornih vektorjev (SVM):
	- išče primere na meji med razredi; išče primere, ki vpenjajo mejo
- k-NN:
	- išče k najbližjih sosedov, pogleda kateri razred prevladuje med njimi
- nevronske mreže:
	- vsak nivo ima svoje uteži, če se na koncu zmotiš na razredu, popraviš uteži - vzvratna kaskadnost
	- išče "kompleksno" preslikavo z ogromno parametri
	- v praksi je to nemogoče razumeti, čeprav se izkaže, da pomaga pri pravilnem napovedovanju

- SVM, NN so bolj kompleksni
- SVM, k-NN dela glede na primere; da razumeš zakaj tako dela, moraš tudi ti poznati vse te primere - težko (nemogoče)

- SVM, k-NN, NN so zelo zahtevni za človeško razumevanje, so numerično zahtevni modeli
- klasifikacijska drevesa in naivni Bayes sta bolj simbolna modela:
	- NB je tudi numerični model, ker računaš pogojne verjetnosti, ampak interpretacija teh numeričnih vrednosti je enostavna

Klasifikacijsko drevo:
- problem katere atribute zares upoštevaš
- najbolj pomembni atribut bo na vrhu drevesa
- uporabno za napovedovanje novih primerov

Kaj je model?
- k-NN je algoritem, ne model:
	- v upanju da bodo novi primeri dovolj podobni starim
	- nima notranje predstavitve podatkov
- model podatkov = predstavitev podatkov v **skrčeni** obliki, zapomni si povzetek podatkov, ki so pokazani z neko strukturo
- povezave: atribut <-> razred
- bolj kot so podatki raznoliki, gosti, večja je potreba po krčenju
- vse podskupine ljudi je treba hkrati opisati z modelom - če je veliko podskupin, bo model primerno večji

- iz enakih podatkov lahko zgradimo različne modele, bolj preproste ali bolj kompleksne
- Okamova britev - model naj bo najbolj enostaven model, ki je pravilen, ker so enostavni modeli bolj robustni na šum, nočeš overfittat na šum

- icicle plot ti dobro pokaže tudi kako velike so posamezne skupine
- drevo na scatter plot dela pravokotne ločnice, ne more delati diagonalnih črt

Klasifikacijska drevesa:
- + preprosta, model je razložjiv
- + lahko sprintaš in uporabiš
- $\pm$ interakcije med atributi (zna bolj na pol):
	- če obstaja par atributov, ki jiju moramo gledati skupaj, ne bo delali najbolje
	- XOR relacije drevo ne bo odkrilo, razen če se naključno pravilno odloči (npr. če slučajno najprej izbere starost, bo potem zbralo spol, sicer tega ne bo najdlo)
- - niso dobri za napovedovanje verjetnosti, da primer pripada razredu, ampak nam samo pove v kateri razred pripada:
	- verjetnost prebere iz distribucije testne množice v listih
	- zaradi ponavadi majhnega števila primerov v listih, težko oceniš prave verjetnosti
- - drevesa so nestabilna:
	- če se podatki malenkost spremenijo, lahko dobimo čisto drugačno drevo z drugačnim vrstnim redom atributov
- - zahteva vrednosti atributov (ne dela z manjkajočimi vrednostmi)

- NN so dokazano najboljši aproksimatorji poljubne funkcije, neko funkcijo lahko poljubno natančno napoveš
- v podatkovni znanosti so nam drevesa bolj všeč, ker nam pomagajo razumeti, kaj je v podatkih (so razložljiva)

Naivni Bayes in nomogram:
- če hočemo dober model, rabimo čim večji vzorec - pri NB je to še toliko bolj pomembno, ker ocenjujemo verjetnosti
- + uporaben na manjkajočih podatkih (vrednostih)
- nomogram:
	- dobro, da lahko sprintaš in uporabiš za napoved
	- verjetnost lahko izračunaš z ravnilom
	- vidiš kako pomemben je posamezen atribut glede na to, koliko dolgo črto ima - daljša črta pomeni, da je bolj pomemben
	- da veš, kje na črti je neka točka, moraš izračunati verjetnost
	- vidimo verjetnost za target class proti vsem ostalim razredom
- + precej zanesljivo računa verjetnosti
- + preprost za uporabo, če naredimo nomogram
- nomogram je primeren tudi za logistične regresije in SVM do neke mere
- nomogram = grafični prikaz računskega modela:
	- da se ga uporabiti na roke
	- izvira iz medicine
	- lahko imaš tudi krive črte za nelinearne modele